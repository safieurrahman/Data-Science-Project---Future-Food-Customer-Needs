{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fav_cuisine_coded\n",
      "0     6\n",
      "1    59\n",
      "2    15\n",
      "3     2\n",
      "4    22\n",
      "5    15\n",
      "6     1\n",
      "7     1\n",
      "8     4\n",
      "Name: fav_cuisine_coded, dtype: int64\n",
      "fav_cuisine_coded\n",
      "1    59\n",
      "2    15\n",
      "4    26\n",
      "5    15\n",
      "Name: fav_cuisine_coded, dtype: int64\n",
      "fav_cuisine_coded\n",
      "1    0.513043\n",
      "2    0.130435\n",
      "4    0.226087\n",
      "5    0.130435\n",
      "Name: fav_cuisine_coded, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#load file\n",
    "df=pd.read_csv(\"food_coded.csv\")\n",
    "df.head(5)\n",
    "\n",
    "#print(X.columns)\n",
    "#print(y.columns)\n",
    "#print(y.head(5))\n",
    "\n",
    "\n",
    "# Simplying data due to low training data avaialble\n",
    "    # Removing 2,6,7 categroy due to low count\n",
    "    # Adding 8 to 4 because indian food also comes under the umbrella of Asian food\n",
    "    # Removing 0 because they are undecided and have low counts\n",
    "\n",
    "print(df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "\n",
    "df.loc[df['fav_cuisine_coded']==8] = 4\n",
    "df=df[df['fav_cuisine_coded'].isin([1,2,4,5])]\n",
    "\n",
    "print(df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "print((df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())/len(df))\n",
    "# Split it into input features and output \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for (i,names) in enumerate(df.columns):\n",
    "    #print (i,\" : \",names)\n",
    "string_cols = [0,7,8,13,16,24,25,28,34,35,42,44,56,60]    \n",
    "#0 cleanign string \n",
    "\n",
    "df_retain = df.drop(df.columns[string_cols], axis = 1) \n",
    "#print(df_retain.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "\n",
    "    # Taqi code will come here, i am just getting the column as it is \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer.fit(df_retain)\n",
    "X = imputer.transform(df_retain)\n",
    "df_sel_clean = pd.DataFrame(X,columns = df_retain.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test-train split using stratification\n",
    "\n",
    "# STRATIFIED SAMPLING \n",
    "X = df_sel_clean.loc[:, df_sel_clean.columns != 'fav_cuisine_coded']\n",
    "y = df_sel_clean[['fav_cuisine_coded']]\n",
    "\n",
    "X_train, tempX_test, y_train, tempY_test = train_test_split(X, y,stratify=y,test_size=0.30)\n",
    "x_val, x_test, y_val, y_test = train_test_split(tempX_test, tempY_test,stratify=tempY_test,test_size=0.64)\n",
    "\n",
    "# Training Set : X_train, y_train\n",
    "# Validation Set : x_val ,  y_val\n",
    "# Testing Set : x_test , y_test\n",
    "\n",
    "\n",
    "#print(y_train.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(y_test.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(y_val.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(X_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_RF(X_train, y_train,x_val ,  y_val):\n",
    "    clf = RandomForestClassifier(n_estimators = 100, random_state=0, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #for feature in zip(X_train.columns, clf.feature_importances_):\n",
    "        #print(feature)\n",
    "    sfm = SelectFromModel(clf, threshold=0.02)\n",
    "    sfm.fit(X_train, y_train)\n",
    "    #for feature_list_index in sfm.get_support(indices=True):\n",
    "        #print(X_train.columns[feature_list_index])\n",
    "    \n",
    "    X_important_train = sfm.transform(X_train)\n",
    "    X_important_val = sfm.transform(x_val)\n",
    "    \n",
    "    clf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "    clf_important.fit(X_important_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(x_val)\n",
    "    acc_full = accuracy_score(y_val, y_pred)\n",
    "    print(acc_full)\n",
    "    \n",
    "    y_important_pred = clf_important.predict(X_important_val)\n",
    "    acc_new = accuracy_score(y_val, y_important_pred)\n",
    "    print(acc_new)\n",
    "    top__cols = list(X_train.columns[sfm.get_support(indices=True)])\n",
    "    #print(top__cols)\n",
    "    return top__cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization\n",
    "\n",
    "    # This will be done once data is cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "    # scatter plots\n",
    "    # histograms\n",
    "    # Box plot \n",
    "    # Line chart etc\n",
    "\n",
    "# Just put whatever you think is related to our model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df_1):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    corrMatrix = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(30,30))\n",
    "    #sns.heatmap(corrMatrix,cmap=ListedColormap(['green','green', 'yellow','red', 'red']), annot=True,linewidths=.5, ax=ax)\n",
    "    sns.heatmap(corrMatrix, annot=True,linewidths=.5, ax=ax)   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_matrix(df_sel_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_feature_sel(X_train,y_train,total_col_selected):\n",
    "    df_train_chi_ind = X_train\n",
    "    df_train_chi_dep = y_train\n",
    "    \n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=20)\n",
    "    fit = bestfeatures.fit(df_train_chi_ind,df_train_chi_dep)\n",
    "    \n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(df_train_chi_ind.columns)\n",
    "    \n",
    "    #concat two dataframes for better visualization \n",
    "    \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    \n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    #print(featureScores.nlargest(20,'Score'))  #print 10 best features\n",
    "    \n",
    "    top_n_columns = featureScores.sort_values('Score',ascending=False).head(total_col_selected).Specs\n",
    "    return top_n_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safie\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_from_model.py:222: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X, y, **fit_params)\n",
      "C:\\Users\\safie\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:15: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "{'turkey_calories', 'ethnic_food', 'on_off_campus', 'waffle_calories', 'comfort_food_reasons_coded', 'tortilla_calories', 'greek_food', 'eating_changes_coded1', 'calories_scone', 'indian_food', 'cuisine', 'thai_food', 'income', 'persian_food', 'comfort_food_reasons_coded.1'}\n"
     ]
    }
   ],
   "source": [
    "top_n_cols = chi_square_feature_sel(X_train,y_train,20)\n",
    "top_n_cols_RF = feature_selection_RF(X_train, y_train,x_val ,  y_val)\n",
    "\n",
    "top_n_cols_final =  set(top_n_cols).intersection(set(top_n_cols_RF))\n",
    "#top_n_cols.append('fav_cuisine_coded')\n",
    "print( set(top_n_cols) & set(top_n_cols_RF))\n",
    "\n",
    "#X_train =  X_train[X_train.columns.intersection(top_n_cols_final)]\n",
    "#x_val =  x_val[x_val.columns.intersection(top_n_cols_final)]\n",
    "#x_test =  x_test[x_test.columns.intersection(top_n_cols_final)]\n",
    "\n",
    "#top_n_cols=list(top_n_cols)\n",
    "#X_train =  X_train[X_train.columns.intersection(top_n_cols)]\n",
    "#x_val =  x_val[x_val.columns.intersection(top_n_cols)]\n",
    "#x_test =  x_test[x_test.columns.intersection(top_n_cols)]\n",
    "\n",
    "X_train =  X_train[X_train.columns.intersection(top_n_cols_RF)]\n",
    "x_val =  x_val[x_val.columns.intersection(top_n_cols_RF)]\n",
    "x_test =  x_test[x_test.columns.intersection(top_n_cols_RF)]\n",
    "\n",
    "\n",
    "#df_sel_features =  df_sel_clean[df_sel_clean.columns.intersection(top_n_cols)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 1. 1. 1. 1. 1. 1. 1. 4. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4782608695652174"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)\n",
    "\n",
    "c_est = softmax_reg.predict(x_val)\n",
    "probs = softmax_reg.predict_proba(x_val)\n",
    "print(c_est)\n",
    "\n",
    "#print(x_val)\n",
    "#print(y_val)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_val, c_est)\n",
    "\n",
    "\n",
    "# GBT (Gradient Boosting Trees)\n",
    "# Logistic Regeression\n",
    "\n",
    "#Ensemble Learning \n",
    "\n",
    "\n",
    "c_est = softmax_reg.predict(x_test)\n",
    "accuracy_score(y_test, c_est)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#cross validation to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for finding right hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function & Thresholding\n",
    "    # Loop through thresholds to get best performance\n",
    "    # If implementing One vs ALl , use baseline performance and compare it with other.\n",
    "    # Precision, Recall, Accuracy, ROC curve, F1 score\n",
    "    # Bin Sampling \n",
    "    # Lift measure\n",
    "    # Migth use R^2 , not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final hold-out sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it is underperforming \n",
    "    #use other model lile (Logistic Regression{One vs All}) <- Try this definitely\n",
    "    # Change feature selection method\n",
    "    # Use l1 unstead of l2 or reduce L2 penalty\n",
    "    \n",
    "# If it is overperforming\n",
    "    # use strict l2\n",
    "    # go back to simple one vs all model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
