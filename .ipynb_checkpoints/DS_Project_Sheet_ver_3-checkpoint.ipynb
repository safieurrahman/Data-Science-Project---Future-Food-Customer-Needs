{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fav_cuisine_coded\n",
      "0     6\n",
      "1    59\n",
      "2    15\n",
      "3     2\n",
      "4    22\n",
      "5    15\n",
      "6     1\n",
      "7     1\n",
      "8     4\n",
      "Name: fav_cuisine_coded, dtype: int64\n",
      "fav_cuisine_coded\n",
      "1    59\n",
      "2    15\n",
      "4    26\n",
      "5    15\n",
      "Name: fav_cuisine_coded, dtype: int64\n",
      "fav_cuisine_coded\n",
      "1    0.513043\n",
      "2    0.130435\n",
      "4    0.226087\n",
      "5    0.130435\n",
      "Name: fav_cuisine_coded, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#load file\n",
    "df=pd.read_csv(\"food_coded.csv\")\n",
    "df.head(5)\n",
    "\n",
    "#print(X.columns)\n",
    "#print(y.columns)\n",
    "#print(y.head(5))\n",
    "\n",
    "\n",
    "# Simplying data due to low training data avaialble\n",
    "    # Removing 2,6,7 categroy due to low count\n",
    "    # Adding 8 to 4 because indian food also comes under the umbrella of Asian food\n",
    "    # Removing 0 because they are undecided and have low counts\n",
    "\n",
    "print(df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "\n",
    "df.loc[df['fav_cuisine_coded']==8] = 4\n",
    "df=df[df['fav_cuisine_coded'].isin([1,2,4,5])]\n",
    "\n",
    "print(df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "print((df.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())/len(df))\n",
    "# Split it into input features and output \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for (i,names) in enumerate(df.columns):\n",
    "    #print (i,\" : \",names)\n",
    "string_cols = [0,7,8,13,16,24,25,28,34,35,42,44,56,60]    \n",
    "#0 cleanign string \n",
    "\n",
    "df_retain = df.drop(df.columns[string_cols], axis = 1) \n",
    "#print(df_retain.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "\n",
    "    # Taqi code will come here, i am just getting the column as it is \n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer.fit(df_retain)\n",
    "X = imputer.transform(df_retain)\n",
    "df_sel_clean = pd.DataFrame(X,columns = df_retain.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test-train split using stratification\n",
    "\n",
    "# STRATIFIED SAMPLING \n",
    "X = df_sel_clean.loc[:, df_sel_clean.columns != 'fav_cuisine_coded']\n",
    "y = df_sel_clean[['fav_cuisine_coded']]\n",
    "\n",
    "X_train, tempX_test, y_train, tempY_test = train_test_split(X, y,stratify=y,test_size=0.30)\n",
    "x_val, x_test, y_val, y_test = train_test_split(tempX_test, tempY_test,stratify=tempY_test,test_size=0.64)\n",
    "\n",
    "# Training Set : X_train, y_train\n",
    "# Validation Set : x_val ,  y_val\n",
    "# Testing Set : x_test , y_test\n",
    "\n",
    "\n",
    "#print(y_train.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(y_test.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(y_val.groupby(['fav_cuisine_coded']).fav_cuisine_coded.count())\n",
    "#print(X_train.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_RF(X_train, y_train,x_val ,  y_val):\n",
    "    clf = RandomForestClassifier(n_estimators = 100, random_state=0, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    #for feature in zip(X_train.columns, clf.feature_importances_):\n",
    "        #print(feature)\n",
    "    sfm = SelectFromModel(clf, threshold=0.02)\n",
    "    sfm.fit(X_train, y_train)\n",
    "    #for feature_list_index in sfm.get_support(indices=True):\n",
    "        #print(X_train.columns[feature_list_index])\n",
    "    \n",
    "    X_important_train = sfm.transform(X_train)\n",
    "    X_important_val = sfm.transform(x_val)\n",
    "    \n",
    "    clf_important = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "    clf_important.fit(X_important_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(x_val)\n",
    "    acc_full = accuracy_score(y_val, y_pred)\n",
    "    print(acc_full)\n",
    "    \n",
    "    y_important_pred = clf_important.predict(X_important_val)\n",
    "    acc_new = accuracy_score(y_val, y_important_pred)\n",
    "    print(acc_new)\n",
    "    top__cols = list(X_train.columns[sfm.get_support(indices=True)])\n",
    "    #print(top__cols)\n",
    "    return top__cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization\n",
    "\n",
    "    # This will be done once data is cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "    # scatter plots\n",
    "    # histograms\n",
    "    # Box plot \n",
    "    # Line chart etc\n",
    "\n",
    "# Just put whatever you think is related to our model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df_1):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    corrMatrix = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(30,30))\n",
    "    #sns.heatmap(corrMatrix,cmap=ListedColormap(['green','green', 'yellow','red', 'red']), annot=True,linewidths=.5, ax=ax)\n",
    "    sns.heatmap(corrMatrix, annot=True,linewidths=.5, ax=ax)   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation_matrix(df_sel_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_feature_sel(X_train,y_train,total_col_selected):\n",
    "    df_train_chi_ind = X_train\n",
    "    df_train_chi_dep = y_train\n",
    "    \n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=20)\n",
    "    fit = bestfeatures.fit(df_train_chi_ind,df_train_chi_dep)\n",
    "    \n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(df_train_chi_ind.columns)\n",
    "    \n",
    "    #concat two dataframes for better visualization \n",
    "    \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    \n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    #print(featureScores.nlargest(20,'Score'))  #print 10 best features\n",
    "    \n",
    "    top_n_columns = featureScores.sort_values('Score',ascending=False).head(total_col_selected).Specs\n",
    "    return top_n_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5833333333333334\n",
      "{'tortilla_calories', 'indian_food', 'ethnic_food', 'turkey_calories', 'waffle_calories', 'comfort_food_reasons_coded.1', 'healthy_feeling', 'comfort_food_reasons_coded', 'calories_chicken', 'life_rewarding', 'on_off_campus', 'thai_food'}\n"
     ]
    }
   ],
   "source": [
    "top_n_cols = chi_square_feature_sel(X_train,y_train,20)\n",
    "top_n_cols_RF = feature_selection_RF(X_train, y_train,x_val,y_val)\n",
    "\n",
    "top_n_cols_final =  set(top_n_cols).intersection(set(top_n_cols_RF))\n",
    "#top_n_cols.append('fav_cuisine_coded')\n",
    "print( set(top_n_cols) & set(top_n_cols_RF))\n",
    "\n",
    "#Commenting others and proceeding with the data that gives best ensemble learning accuracy on test set\n",
    "\n",
    "# X_train =  X_train[X_train.columns.intersection(top_n_cols_final)]\n",
    "# x_val =  x_val[x_val.columns.intersection(top_n_cols_final)]\n",
    "# x_test =  x_test[x_test.columns.intersection(top_n_cols_final)]\n",
    "\n",
    "# top_n_cols=list(top_n_cols)\n",
    "# X_train =  X_train[X_train.columns.intersection(top_n_cols)]\n",
    "# x_val =  x_val[x_val.columns.intersection(top_n_cols)]\n",
    "# x_test =  x_test[x_test.columns.intersection(top_n_cols)]\n",
    "\n",
    "X_train =  X_train[X_train.columns.intersection(top_n_cols_RF)]\n",
    "x_val =  x_val[x_val.columns.intersection(top_n_cols_RF)]\n",
    "x_test =  x_test[x_test.columns.intersection(top_n_cols_RF)]\n",
    "\n",
    "#df_sel_features =  df_sel_clean[df_sel_clean.columns.intersection(top_n_cols)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 4. 4. 1. 2. 1. 2. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.391304347826087"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(X_train, y_train)\n",
    "\n",
    "c_est = softmax_reg.predict(x_val)\n",
    "probs = softmax_reg.predict_proba(x_val)\n",
    "print(c_est)\n",
    "\n",
    "#print(x_val)\n",
    "#print(y_val)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#accuracy_score(y_val, c_est)\n",
    "\n",
    "\n",
    "# GBT (Gradient Boosting Trees)\n",
    "# Logistic Regeression\n",
    "\n",
    "#Ensemble Learning \n",
    "\n",
    "\n",
    "c_est = softmax_reg.predict(x_test)\n",
    "accuracy_score(y_test, c_est)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 0.001, 'multi_class': 'multinomial', 'penalty': 'l2', 'solver': 'saga'}\n",
      "accuracy : 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5217391304347826"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid search for finding right hyperparameter for Multinomial and One vs All Regression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid={\"C\":np.logspace(-3,3,7),\"multi_class\":[\"multinomial\",\"ovr\",], \"penalty\":[\"l1\",\"l2\"], \"solver\":[\"lbfgs\",\"saga\",\"newton-cg\"]} # l1 lasso l2 ridge\n",
    "logreg=LogisticRegression(max_iter= 1000, random_state=42)\n",
    "logreg_cv=GridSearchCV(logreg, grid, cv=10, scoring=acc_scorer)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "\n",
    "\n",
    "# Set the logreg_best to the best combination of parameters\n",
    "logreg_best = logreg_cv.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "logreg_best.fit(X_train, y_train)\n",
    "\n",
    "logreg_est = logreg_best.predict(x_test)\n",
    "accuracy_score(y_test, logreg_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'criterion': 'friedman_mse', 'learning_rate': 0.025, 'loss': 'deviance', 'max_depth': 3, 'max_features': 'log2', 'n_estimators': 10}\n",
      "accuracy : 0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5217391304347826"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid search for finding right hyperparameter for Gradient Boost Classification\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"max_depth\":[3,5,8],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     \"n_estimators\":[10]\n",
    "#     }\n",
    "\n",
    "#Using a subset of parameters for early computation, more parameters could be added from above sample\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grad_cv = GridSearchCV(GradientBoostingClassifier(random_state=42), parameters, cv=10, scoring=acc_scorer)\n",
    "grad_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",grad_cv.best_params_)\n",
    "print(\"accuracy :\",grad_cv.best_score_)\n",
    "\n",
    "# Set the grad_best to the best combination of parameters\n",
    "grad_best = grad_cv.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "grad_best.fit(X_train, y_train)\n",
    "\n",
    "grad_est = grad_best.predict(x_test)\n",
    "accuracy_score(y_test, grad_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'learning_rate': 0.075, 'n_estimators': 60}\n",
      "accuracy : 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4782608695652174"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grid search for finding right hyperparameter for Ada Boost Classification\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"n_estimators\":[10, 20, 30, 40 ,50, 60, 70, 80, 90, 100]\n",
    "    }\n",
    "\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "ada_cv = GridSearchCV(AdaBoostClassifier(random_state=42), parameters, cv=10, scoring=acc_scorer)\n",
    "ada_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",ada_cv.best_params_)\n",
    "print(\"accuracy :\",ada_cv.best_score_)\n",
    "\n",
    "# Set the ada_best to the best combination of parameters\n",
    "ada_best = ada_cv.best_estimator_\n",
    "\n",
    "# Fit the best algorithm to the data. \n",
    "ada_best.fit(X_train, y_train)\n",
    "\n",
    "ada_est = ada_best.predict(x_test)\n",
    "accuracy_score(y_test, ada_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "#Ensemble learning implementation, with Logistic Regression (Multinomial or OVA), Gradient and Ada-Boost \n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('LRM', logreg_best), ('GDB', grad_best), ('ADB', ada_best)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "preds = voting_clf.predict(x_test)\n",
    "acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy is: \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19  20  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  21  22  23  25  43  44  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  45  46  47  66  67  69  72  73  74  75  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
      "  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  68  70  71  80  84  87\n",
      "  94  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112\n",
      " 113 114]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 81 82 83 85 86 88 89 90 91 92 93 95]\n"
     ]
    }
   ],
   "source": [
    "#test-train split using StratifiedKFold that also shuffles the data\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit\n",
    "\n",
    "X = df_sel_clean.loc[:, df_sel_clean.columns != 'fav_cuisine_coded']\n",
    "y = df_sel_clean[['fav_cuisine_coded']]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "for train, test in cv.split(X,y):\n",
    "    print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Function for visualization of K-fold Stratified Sampling with 5 splits\n",
    "\n",
    "def plot_cv(cv, features, labels):\n",
    "    masks = []\n",
    "    for train, test in cv.split(features, labels):\n",
    "        mask = np.zeros(len(labels), dtype=bool)\n",
    "        mask[test] = 1\n",
    "        masks.append(mask)\n",
    "    \n",
    "    plt.matshow(masks, cmap='gray_r')\n",
    "    \n",
    "plot_cv(StratifiedKFold(n_splits=5),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56521739 0.56521739 0.52173913 0.56521739 0.52173913]\n",
      "Mean: 0.5478260869565217\n",
      "Standard deviation: 0.02129991080681023\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(voting_clf, X, y, cv=cv)\n",
    "print(scores)\n",
    "print(\"Mean:\", np.mean(scores.mean()))\n",
    "print(\"Standard deviation:\", np.mean(scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0, 11],\n",
       "        [ 0, 12]],\n",
       "\n",
       "       [[20,  0],\n",
       "        [ 3,  0]],\n",
       "\n",
       "       [[18,  0],\n",
       "        [ 5,  0]],\n",
       "\n",
       "       [[20,  0],\n",
       "        [ 3,  0]]], dtype=int64)"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "#from sklearn.metrics import precision_score\n",
    "\n",
    "#precision_score(y_test,preds, average='macro')\n",
    "\n",
    "multilabel_confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search for finding right hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function & Thresholding\n",
    "    # Loop through thresholds to get best performance\n",
    "    # If implementing One vs ALl , use baseline performance and compare it with other.\n",
    "    # Precision, Recall, Accuracy, ROC curve, F1 score\n",
    "    # Bin Sampling \n",
    "    # Lift measure\n",
    "    # Migth use R^2 , not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final hold-out sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it is underperforming \n",
    "    #use other model lile (Logistic Regression{One vs All}) <- Try this definitely\n",
    "    # Change feature selection method\n",
    "    # Use l1 unstead of l2 or reduce L2 penalty\n",
    "    \n",
    "# If it is overperforming\n",
    "    # use strict l2\n",
    "    # go back to simple one vs all model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
